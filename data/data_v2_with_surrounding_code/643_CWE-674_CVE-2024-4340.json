{
    "cve_id": "CVE-2024-4340",
    "cve_description": "Passing a heavily nested list to sqlparse.parse() leads to a Denial of Service due to RecursionError.\n\n",
    "cve_publish_date": "2024-04-30T15:15Z",
    "cwe_id": "CWE-674",
    "cwe_name": "Uncontrolled Recursion",
    "cwe_description": "The product does not properly control the amount of recursion that takes place,  consuming excessive resources, such as allocated memory or the program stack.",
    "commit_message": "Raise SQLParseError instead of RecursionError.",
    "type_of_change": "Modification",
    "changes": [
        {
            "filename_of_changes": "sql.py",
            "code_language": "Python",
            "number_of_lines_added_for_mitigation": "9",
            "number_of_lines_deleted_vulnerable_to_cve": "5",
            "Code_with_highlighted_vulnerability_lines": [
                "// Line 202:             end = idx + len(token.value)",
                "// Line 203:             if idx <= offset < end:",
                "// Line 204:                 return token",
                "// Line 205:             idx = end",
                "// Line 206: ",
                "// Line 207:     def flatten(self):",
                "// Line 208:         \"\"\"Generator yielding ungrouped tokens.",
                "// Line 209: ",
                "// Line 210:         This method is recursively called for all child tokens.",
                "// Line 211:         \"\"\"",
                "// vulnerable line: 212: for token in self.tokens:",
                "// vulnerable line: 213: if token.is_group:",
                "// vulnerable line: 214: yield from token.flatten()",
                "// vulnerable line: 215: else:",
                "// vulnerable line: 216: yield token",
                "// Line 217: ",
                "// Line 218:     def get_sublists(self):",
                "// vulnerable line: 219: for token in self.tokens:",
                "// vulnerable line: 220: if token.is_group:",
                "// vulnerable line: 221: yield token",
                "// Line 222: ",
                "// Line 223:     @property",
                "// Line 224:     def _groupable_tokens(self):",
                "// Line 225:         return self.tokens",
                "// Line 226: ",
                "// Line 227:     def _token_matching(self, funcs, start=0, end=None, reverse=False):",
                "// Line 228:         \"\"\"next token that match functions\"\"\"",
                "// Line 229:         if start is None:",
                "// Line 230:             return None",
                "// Line 231: ",
                "// Line 232:         if not isinstance(funcs, (list, tuple)):",
                "// Line 233:             funcs = (funcs,)",
                "// Line 234: ",
                "// Line 235:         if reverse:",
                "// Line 236:             assert end is None",
                "// Line 237:             indexes = range(start - 2, -1, -1)",
                "// vulnerable line: 238: else:",
                "// Line 239:             if end is None:",
                "// Line 240:                 end = len(self.tokens)",
                "// Line 241:             indexes = range(start, end)",
                "// Line 242:         for idx in indexes:",
                "// Line 243:             token = self.tokens[idx]",
                "// Line 244:             for func in funcs:",
                "// Line 245:                 if func(token):",
                "// Line 246:                     return idx, token",
                "// Line 247:         return None, None",
                "// Line 248: ",
                "// Line 316:         # while skip_ws and tokens and tokens[-1].is_whitespace:",
                "// Line 317:         #     tokens = tokens[:-1]",
                "// Line 318: ",
                "// Line 319:         if extend and isinstance(start, grp_cls):",
                "// Line 320:             subtokens = self.tokens[start_idx + 1:end_idx]",
                "// Line 321: ",
                "// Line 322:             grp = start",
                "// Line 323:             grp.tokens.extend(subtokens)",
                "// Line 324:             del self.tokens[start_idx + 1:end_idx]",
                "// Line 325:             grp.value = str(start)",
                "// vulnerable line: 326: else:",
                "// Line 327:             subtokens = self.tokens[start_idx:end_idx]",
                "// Line 328:             grp = grp_cls(subtokens)",
                "// Line 329:             self.tokens[start_idx:end_idx] = [grp]",
                "// Line 330:             grp.parent = self",
                "// Line 331: ",
                "// Line 332:         for token in subtokens:",
                "// Line 333:             token.parent = grp",
                "// Line 334: ",
                "// Line 335:         return grp",
                "// Line 336: ",
                "// Line 342:         self.tokens.insert(where, token)",
                "// Line 343: ",
                "// Line 344:     def insert_after(self, where, token, skip_ws=True):",
                "// Line 345:         \"\"\"Inserts *token* after *where*.\"\"\"",
                "// Line 346:         if not isinstance(where, int):",
                "// Line 347:             where = self.token_index(where)",
                "// Line 348:         nidx, next_ = self.token_next(where, skip_ws=skip_ws)",
                "// Line 349:         token.parent = self",
                "// Line 350:         if next_ is None:",
                "// Line 351:             self.tokens.append(token)",
                "// vulnerable line: 352: else:",
                "// Line 353:             self.tokens.insert(nidx, token)",
                "// Line 354: ",
                "// Line 355:     def has_alias(self):",
                "// Line 356:         \"\"\"Returns ``True`` if an alias is present.\"\"\"",
                "// Line 357:         return self.get_alias() is not None",
                "// Line 358: ",
                "// Line 359:     def get_alias(self):",
                "// Line 360:         \"\"\"Returns the alias for this identifier or ``None``.\"\"\"",
                "// Line 361:         return None",
                "// Line 362: ",
                "// Line 457:         return next_.value if next_ else None",
                "// Line 458: ",
                "// Line 459:     def get_ordering(self):",
                "// Line 460:         \"\"\"Returns the ordering or ``None`` as uppercase string.\"\"\"",
                "// Line 461:         _, ordering = self.token_next_by(t=T.Keyword.Order)",
                "// Line 462:         return ordering.normalized if ordering else None",
                "// Line 463: ",
                "// Line 464:     def get_array_indices(self):",
                "// Line 465:         \"\"\"Returns an iterator of index token lists\"\"\"",
                "// Line 466: ",
                "// vulnerable line: 467: for token in self.tokens:",
                "// Line 468:             if isinstance(token, SquareBrackets):",
                "// Line 469:                 # Use [1:-1] index to discard the square brackets",
                "// Line 470:                 yield token.tokens[1:-1]",
                "// Line 471: ",
                "// Line 472: ",
                "// Line 473: class IdentifierList(TokenList):",
                "// Line 474:     \"\"\"A list of :class:`~sqlparse.sql.Identifier`'s.\"\"\"",
                "// Line 475: ",
                "// Line 476:     def get_identifiers(self):",
                "// Line 477:         \"\"\"Returns the identifiers.",
                "// Line 478: ",
                "// Line 479:         Whitespaces and punctuations are not included in this generator.",
                "// Line 480:         \"\"\"",
                "// vulnerable line: 481: for token in self.tokens:",
                "// Line 482:             if not (token.is_whitespace or token.match(T.Punctuation, ',')):",
                "// vulnerable line: 483: yield token",
                "// Line 484: ",
                "// Line 485: ",
                "// Line 486: class TypedLiteral(TokenList):",
                "// Line 487:     \"\"\"A typed literal, such as \"date '2001-09-28'\" or \"interval '2 hours'\".\"\"\"",
                "// Line 488:     M_OPEN = [(T.Name.Builtin, None), (T.Keyword, \"TIMESTAMP\")]",
                "// Line 489:     M_CLOSE = T.String.Single, None",
                "// Line 490:     M_EXTEND = T.Keyword, (\"DAY\", \"HOUR\", \"MINUTE\", \"MONTH\", \"SECOND\", \"YEAR\")",
                "// Line 491: ",
                "// Line 492: ",
                "// Line 493: class Parenthesis(TokenList):",
                "// Line 573:         \"\"\"Returns a list of 2-tuples (condition, value).",
                "// Line 574: ",
                "// Line 575:         If an ELSE exists condition is None.",
                "// Line 576:         \"\"\"",
                "// Line 577:         CONDITION = 1",
                "// Line 578:         VALUE = 2",
                "// Line 579: ",
                "// Line 580:         ret = []",
                "// Line 581:         mode = CONDITION",
                "// Line 582: ",
                "// vulnerable line: 583: for token in self.tokens:",
                "// Line 584:             # Set mode from the current statement",
                "// Line 585:             if token.match(T.Keyword, 'CASE'):",
                "// Line 586:                 continue",
                "// Line 587: ",
                "// Line 588:             elif skip_ws and token.ttype in T.Whitespace:",
                "// Line 589:                 continue",
                "// Line 590: ",
                "// Line 591:             elif token.match(T.Keyword, 'WHEN'):",
                "// Line 592:                 ret.append(([], []))",
                "// Line 593:                 mode = CONDITION"
            ]
        },
        {
            "filename_of_changes": "test_regressions.py",
            "code_language": "Python",
            "number_of_lines_added_for_mitigation": "16",
            "number_of_lines_deleted_vulnerable_to_cve": "1",
            "Code_with_highlighted_vulnerability_lines": [
                "// Line 464: ",
                "// Line 465: def test_copy_issue672():",
                "// Line 466:     p = sqlparse.parse('select * from foo')[0]",
                "// Line 467:     copied = copy.deepcopy(p)",
                "// Line 468:     assert str(p) == str(copied)",
                "// Line 469: ",
                "// Line 470: ",
                "// Line 471: def test_primary_key_issue740():",
                "// Line 472:     p = sqlparse.parse('PRIMARY KEY')[0]",
                "// Line 473:     assert len(p.tokens) == 1",
                "// vulnerable line: 474: assert p.tokens[0].ttype == T.Keyword"
            ]
        }
    ]
}