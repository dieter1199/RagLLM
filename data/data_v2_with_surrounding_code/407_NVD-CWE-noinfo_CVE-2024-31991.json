{
    "cve_id": "CVE-2024-31991",
    "cve_description": "Mealie is a self hosted recipe manager and meal planner. Prior to 1.4.0, the safe_scrape_html function utilizes a user-controlled URL to issue a request to a remote server. Based on the content of the response, it will either parse the content or disregard it. This function, nor those that call it, add any restrictions on the URL that can be provided, nor is it restricted to being an FQDN (i.e., an IP address can be provided). As this functionâ€™s return will be handled differently by its caller depending on the response, it is possible for an attacker to use this functionality to positively identify HTTP(s) servers on the local network with any IP/port combination. This issue can result in any authenticated user being able to map HTTP servers on a local network that the Mealie service has access to. Note that by default any user can create an account on a Mealie server, and that the default changeme@example.com user is available with its hard-coded password. This vulnerability is fixed in 1.4.0.",
    "cve_publish_date": "2024-04-19T21:15Z",
    "cwe_id": "NVD-CWE-noinfo",
    "cwe_name": "Insufficient Information",
    "cwe_description": "There is insufficient information about the issue to classify it; details are unkown or unspecified.",
    "commit_message": "security: gh security recs (#3368)\n\n* change ALLOW_SIGNUP to default to false\r\n\r\n* add 1.4.0 tag for OIDC docs\r\n\r\n* new notes on security inline with security/policy review\r\n\r\n* safer transport for external requests\r\n\r\n* fix linter errors\r\n\r\n* docs: Tidy up wording/formatting\r\n\r\n* fix request errors\r\n\r\n* whoops\r\n\r\n* fix implementation with std lib\r\n\r\n* format\r\n\r\n* Remove check on netloc_parts. It only includes URL after any @\r\n\r\n---------\r\n\r\nCo-authored-by: boc-the-git <3479092+boc-the-git@users.noreply.github.com>\r\nCo-authored-by: Brendan <b.oconnell14@gmail.com>",
    "type_of_change": "Addition",
    "changes": [
        {
            "filename_of_changes": "__init__.py",
            "code_language": "Python",
            "number_of_lines_added_for_mitigation": "7",
            "number_of_lines_deleted_vulnerable_to_cve": "0",
            "Code_with_highlighted_vulnerability_lines": []
        },
        {
            "filename_of_changes": "conftest.py",
            "code_language": "Python",
            "number_of_lines_added_for_mitigation": "1",
            "number_of_lines_deleted_vulnerable_to_cve": "0",
            "Code_with_highlighted_vulnerability_lines": []
        },
        {
            "filename_of_changes": "recipe_data_service.py",
            "code_language": "Python",
            "number_of_lines_added_for_mitigation": "17",
            "number_of_lines_deleted_vulnerable_to_cve": "27",
            "Code_with_highlighted_vulnerability_lines": [
                "// Line 1: import asyncio",
                "// Line 2: import shutil",
                "// Line 3: from pathlib import Path",
                "// Line 4: ",
                "// Line 5: from httpx import AsyncClient, Response",
                "// Line 6: from pydantic import UUID4",
                "// Line 7: ",
                "// vulnerable line: 8: from mealie.pkgs import img",
                "// Line 9: from mealie.schema.recipe.recipe import Recipe",
                "// Line 10: from mealie.services._base_service import BaseService",
                "// Line 11: ",
                "// Line 12: _FIREFOX_UA = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:86.0) Gecko/20100101 Firefox/86.0\"",
                "// Line 13: ",
                "// Line 14: ",
                "// Line 15: async def gather_with_concurrency(n, *coros, ignore_exceptions=False):",
                "// Line 16:     semaphore = asyncio.Semaphore(n)",
                "// Line 17: ",
                "// Line 18:     async def sem_coro(coro):",
                "// Line 25:     return results",
                "// Line 26: ",
                "// Line 27: ",
                "// Line 28: async def largest_content_len(urls: list[str]) -> tuple[str, int]:",
                "// Line 29:     largest_url = \"\"",
                "// Line 30:     largest_len = 0",
                "// Line 31: ",
                "// Line 32:     async def do(client: AsyncClient, url: str) -> Response:",
                "// Line 33:         return await client.head(url, headers={\"User-Agent\": _FIREFOX_UA})",
                "// Line 34: ",
                "// vulnerable line: 35: async with AsyncClient() as client:",
                "// Line 36:         tasks = [do(client, url) for url in urls]",
                "// vulnerable line: 37: responses: list[Response] = await gather_with_concurrency(10, *tasks, ignore_exceptions=True)",
                "// Line 38:         for response in responses:",
                "// Line 39:             len_int = int(response.headers.get(\"Content-Length\", 0))",
                "// Line 40:             if len_int > largest_len:",
                "// Line 41:                 largest_url = str(response.url)",
                "// Line 42:                 largest_len = len_int",
                "// Line 43: ",
                "// Line 44:     return largest_url, largest_len",
                "// Line 45: ",
                "// Line 46: ",
                "// Line 47: class NotAnImageError(Exception):",
                "// vulnerable line: 48: pass",
                "// Line 49: ",
                "// Line 50: ",
                "// Line 51: class InvalidDomainError(Exception):",
                "// vulnerable line: 52: pass",
                "// Line 53: ",
                "// Line 54: ",
                "// Line 55: class RecipeDataService(BaseService):",
                "// Line 56:     minifier: img.ABCMinifier",
                "// Line 57: ",
                "// Line 58:     def __init__(self, recipe_id: UUID4, group_id: UUID4 | None = None) -> None:",
                "// vulnerable line: 59: \"\"\"",
                "// Line 60:         RecipeDataService is a service that consolidates the reading/writing actions related",
                "// Line 61:         to assets, and images for a recipe.",
                "// vulnerable line: 62: \"\"\"",
                "// Line 63:         super().__init__()",
                "// Line 64: ",
                "// Line 65:         self.recipe_id = recipe_id",
                "// Line 66:         self.slug = group_id",
                "// Line 67:         self.minifier = img.PillowMinifier(purge=True, logger=self.logger)",
                "// Line 68: ",
                "// Line 69:         self.dir_data = Recipe.directory_from_id(self.recipe_id)",
                "// Line 70:         self.dir_image = self.dir_data.joinpath(\"images\")",
                "// Line 71:         self.dir_image_timeline = self.dir_image.joinpath(\"timeline\")",
                "// Line 72:         self.dir_assets = self.dir_data.joinpath(\"assets\")",
                "// Line 94:             with open(image_path, \"ab\") as f:",
                "// Line 95:                 f.write(file_data)",
                "// Line 96:         else:",
                "// Line 97:             with open(image_path, \"ab\") as f:",
                "// Line 98:                 shutil.copyfileobj(file_data, f)",
                "// Line 99: ",
                "// Line 100:         self.minifier.minify(image_path)",
                "// Line 101: ",
                "// Line 102:         return image_path",
                "// Line 103: ",
                "// vulnerable line: 104: @staticmethod",
                "// vulnerable line: 105: def _validate_image_url(url: str) -> bool:",
                "// vulnerable line: 106: # sourcery skip: invert-any-all, use-any",
                "// vulnerable line: 107: \"\"\"",
                "// vulnerable line: 108: Validates that the URL is of an allowed source and restricts certain sources to prevent",
                "// vulnerable line: 109: malicious images from being downloaded.",
                "// vulnerable line: 110: \"\"\"",
                "// vulnerable line: 111: invalid_domains = {\"127.0.0.1\", \"localhost\"}",
                "// vulnerable line: 112: for domain in invalid_domains:",
                "// vulnerable line: 113: if domain in url:",
                "// vulnerable line: 114: return False",
                "// Line 115: ",
                "// vulnerable line: 116: return True",
                "// Line 117: ",
                "// vulnerable line: 118: async def scrape_image(self, image_url) -> None:",
                "// Line 119:         self.logger.info(f\"Image URL: {image_url}\")",
                "// Line 120: ",
                "// vulnerable line: 121: if not self._validate_image_url(image_url):",
                "// vulnerable line: 122: self.logger.error(f\"Invalid image URL: {image_url}\")",
                "// vulnerable line: 123: raise InvalidDomainError(f\"Invalid domain: {image_url}\")",
                "// Line 124: ",
                "// Line 125:         if isinstance(image_url, str):  # Handles String Types",
                "// vulnerable line: 126: pass",
                "// Line 127: ",
                "// Line 128:         elif isinstance(image_url, list):  # Handles List Types",
                "// Line 129:             # Multiple images have been defined in the schema - usually different resolutions",
                "// Line 130:             # Typically would be in smallest->biggest order, but can't be certain so test each.",
                "// Line 131:             # 'Google will pick the best image to display in Search results based on the aspect ratio and resolution.'",
                "// vulnerable line: 132: image_url, _ = await largest_content_len(image_url)",
                "// Line 133: ",
                "// Line 134:         elif isinstance(image_url, dict):  # Handles Dictionary Types",
                "// Line 135:             for key in image_url:",
                "// Line 136:                 if key == \"url\":",
                "// vulnerable line: 137: image_url = image_url.get(\"url\")",
                "// Line 138: ",
                "// vulnerable line: 139: ext = image_url.split(\".\")[-1]",
                "// Line 140: ",
                "// Line 141:         if ext not in img.IMAGE_EXTENSIONS:",
                "// Line 142:             ext = \"jpg\"  # Guess the extension",
                "// Line 143: ",
                "// Line 144:         file_name = f\"{str(self.recipe_id)}.{ext}\"",
                "// Line 145:         file_path = Recipe.directory_from_id(self.recipe_id).joinpath(\"images\", file_name)",
                "// Line 146: ",
                "// vulnerable line: 147: async with AsyncClient() as client:",
                "// Line 148:             try:",
                "// vulnerable line: 149: r = await client.get(image_url, headers={\"User-Agent\": _FIREFOX_UA})",
                "// Line 150:             except Exception:",
                "// Line 151:                 self.logger.exception(\"Fatal Image Request Exception\")",
                "// Line 152:                 return None",
                "// Line 153: ",
                "// Line 154:             if r.status_code != 200:",
                "// Line 155:                 # TODO: Probably should throw an exception in this case as well, but before these changes",
                "// Line 156:                 # we were returning None if it failed anyways.",
                "// Line 157:                 return None",
                "// Line 158: ",
                "// Line 159:             content_type = r.headers.get(\"content-type\", \"\")"
            ]
        },
        {
            "filename_of_changes": "scraper.py",
            "code_language": "Python",
            "number_of_lines_added_for_mitigation": "1",
            "number_of_lines_deleted_vulnerable_to_cve": "1",
            "Code_with_highlighted_vulnerability_lines": [
                "// Line 36:     if not new_recipe:",
                "// Line 37:         raise HTTPException(status.HTTP_400_BAD_REQUEST, {\"details\": ParserErrors.BAD_RECIPE_DATA.value})",
                "// Line 38: ",
                "// Line 39:     new_recipe.id = uuid4()",
                "// Line 40:     logger = get_logger()",
                "// Line 41:     logger.debug(f\"Image {new_recipe.image}\")",
                "// Line 42: ",
                "// Line 43:     recipe_data_service = RecipeDataService(new_recipe.id)",
                "// Line 44: ",
                "// Line 45:     try:",
                "// vulnerable line: 46: await recipe_data_service.scrape_image(new_recipe.image)",
                "// Line 47: ",
                "// Line 48:         if new_recipe.name is None:",
                "// Line 49:             new_recipe.name = \"Untitled\"",
                "// Line 50: ",
                "// Line 51:         new_recipe.slug = slugify(new_recipe.name)",
                "// Line 52:         new_recipe.image = cache.new_key(4)",
                "// Line 53:     except Exception as e:",
                "// Line 54:         recipe_data_service.logger.exception(f\"Error Scraping Image: {e}\")",
                "// Line 55:         new_recipe.image = \"no image\"",
                "// Line 56: "
            ]
        },
        {
            "filename_of_changes": "scraper_strategies.py",
            "code_language": "Python",
            "number_of_lines_added_for_mitigation": "2",
            "number_of_lines_deleted_vulnerable_to_cve": "1",
            "Code_with_highlighted_vulnerability_lines": [
                "// Line 24: class ForceTimeoutException(Exception):",
                "// Line 25:     pass",
                "// Line 26: ",
                "// Line 27: ",
                "// Line 28: async def safe_scrape_html(url: str) -> str:",
                "// Line 29:     \"\"\"",
                "// Line 30:     Scrapes the html from a url but will cancel the request",
                "// Line 31:     if the request takes longer than 15 seconds. This is used to mitigate",
                "// Line 32:     DDOS attacks from users providing a url with arbitrary large content.",
                "// Line 33:     \"\"\"",
                "// vulnerable line: 34: async with AsyncClient() as client:",
                "// Line 35:         html_bytes = b\"\"",
                "// Line 36:         async with client.stream(\"GET\", url, timeout=SCRAPER_TIMEOUT, headers={\"User-Agent\": _FIREFOX_UA}) as resp:",
                "// Line 37:             start_time = time.time()",
                "// Line 38: ",
                "// Line 39:             async for chunk in resp.aiter_bytes(chunk_size=1024):",
                "// Line 40:                 html_bytes += chunk",
                "// Line 41: ",
                "// Line 42:                 if time.time() - start_time > SCRAPER_TIMEOUT:",
                "// Line 43:                     raise ForceTimeoutException()",
                "// Line 44: "
            ]
        },
        {
            "filename_of_changes": "settings.py",
            "code_language": "Python",
            "number_of_lines_added_for_mitigation": "1",
            "number_of_lines_deleted_vulnerable_to_cve": "1",
            "Code_with_highlighted_vulnerability_lines": [
                "// Line 40:     API_DOCS: bool = True",
                "// Line 41:     TOKEN_TIME: int = 48",
                "// Line 42:     \"\"\"time in hours\"\"\"",
                "// Line 43: ",
                "// Line 44:     SECRET: str",
                "// Line 45:     LOG_LEVEL: str = \"INFO\"",
                "// Line 46:     \"\"\"corresponds to standard Python log levels\"\"\"",
                "// Line 47: ",
                "// Line 48:     GIT_COMMIT_HASH: str = \"unknown\"",
                "// Line 49: ",
                "// vulnerable line: 50: ALLOW_SIGNUP: bool = True",
                "// Line 51: ",
                "// Line 52:     # ===============================================",
                "// Line 53:     # Security Configuration",
                "// Line 54: ",
                "// Line 55:     SECURITY_MAX_LOGIN_ATTEMPTS: int = 5",
                "// Line 56:     SECURITY_USER_LOCKOUT_TIME: int = 24",
                "// Line 57:     \"time in hours\"",
                "// Line 58: ",
                "// Line 59:     @field_validator(\"BASE_URL\")",
                "// Line 60:     @classmethod"
            ]
        }
    ]
}