{
    "cve_id": "CVE-2024-34359",
    "cve_description": "llama-cpp-python is the Python bindings for llama.cpp. `llama-cpp-python` depends on class `Llama` in `llama.py` to load `.gguf` llama.cpp or Latency Machine Learning Models. The `__init__` constructor built in the `Llama` takes several parameters to configure the loading and running of the model. Other than `NUMA, LoRa settings`, `loading tokenizers,` and `hardware settings`, `__init__` also loads the `chat template` from targeted `.gguf` 's Metadata and furtherly parses it to `llama_chat_format.Jinja2ChatFormatter.to_chat_handler()` to construct the `self.chat_handler` for this model. Nevertheless, `Jinja2ChatFormatter` parse the `chat template` within the Metadate with sandbox-less `jinja2.Environment`, which is furthermore rendered in `__call__` to construct the `prompt` of interaction. This allows `jinja2` Server Side Template Injection which leads to remote code execution by a carefully constructed payload.",
    "cve_publish_date": "2024-05-14T15:38Z",
    "cwe_id": "NVD-CWE-noinfo",
    "cwe_name": "Insufficient Information",
    "cwe_description": "There is insufficient information about the issue to classify it; details are unkown or unspecified.",
    "commit_message": "Merge pull request from GHSA-56xg-wfcc-g829\n\nCo-authored-by: Andrei <abetlen@gmail.com>",
    "type_of_change": "Modification",
    "changes": [
        {
            "filename_of_changes": "llama_chat_format.py",
            "code_language": "Python",
            "number_of_lines_added_for_mitigation": "2",
            "number_of_lines_deleted_vulnerable_to_cve": "1",
            "Code_with_highlighted_vulnerability_lines": [
                "// Line 191:         add_generation_prompt: bool = True,",
                "// Line 192:         stop_token_ids: Optional[List[int]] = None,",
                "// Line 193:     ):",
                "// Line 194:         \"\"\"A chat formatter that uses jinja2 templates to format the prompt.\"\"\"",
                "// Line 195:         self.template = template",
                "// Line 196:         self.eos_token = eos_token",
                "// Line 197:         self.bos_token = bos_token",
                "// Line 198:         self.add_generation_prompt = add_generation_prompt",
                "// Line 199:         self.stop_token_ids = set(stop_token_ids) if stop_token_ids is not None else None",
                "// Line 200: ",
                "// vulnerable line: 201: self._environment = jinja2.Environment(",
                "// Line 202:             loader=jinja2.BaseLoader(),",
                "// Line 203:             trim_blocks=True,",
                "// Line 204:             lstrip_blocks=True,",
                "// Line 205:         ).from_string(self.template)",
                "// Line 206: ",
                "// Line 207:     def __call__(",
                "// Line 208:         self,",
                "// Line 209:         *,",
                "// Line 210:         messages: List[llama_types.ChatCompletionRequestMessage],",
                "// Line 211:         functions: Optional[List[llama_types.ChatCompletionFunction]] = None,"
            ]
        }
    ]
}