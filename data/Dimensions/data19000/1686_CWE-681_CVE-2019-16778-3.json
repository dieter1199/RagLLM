{
    "cve_id": "CVE-2019-16778",
    "cve_description": "In TensorFlow before 1.15, a heap buffer overflow in UnsortedSegmentSum can be produced when the Index template argument is int32. In this case data_size and num_segments fields are truncated from int64 to int32 and can produce negative numbers, resulting in accessing out of bounds heap memory. This is unlikely to be exploitable and was detected and fixed internally in TensorFlow 1.15 and 2.0.",
    "cve_publish_date": "2019-12-16",
    "cwe_id": "CWE-681",
    "cwe_name": "Incorrect Conversion between Numeric Types",
    "cwe_description": "When converting from one data type to another, such as long to integer, data can be omitted or translated in a way that produces unexpected values. If the resulting values are used in a sensitive context, then dangerous behaviors may occur.",
    "commit_message": "Fix heap buffer overflow in UnsortedSegmentSum.\n\nWhen Index=int32, data_size and num_segments were truncated from int64 to int32. This truncation can produce negative numbers, which causes UnsortedSegmentFunctor to access out of bounds memory.\n\nAlso:\n- Switches some indexing calculations to int64 to avoid signed integer overflow when either the input or output tensors have more than 2**31 - 1 elements.\n- Fixes a range check error in the GPU kernel. The segment ID was checked against an upper bound measured in elements, not segments.\nPiperOrigin-RevId: 256451663",
    "type_of_change": "Modification",
    "filename_of_changes": "segment_reduction_ops_gpu.cu.cc",
    "code_language": "C++",
    "number_of_lines_added_for_mitigation": "21",
    "number_of_lines_deleted_vulnerable_to_cve": "20",
    "vulnerable_lines": [
        "// Line_Reference 109: __global__ void UnsortedSegmentCustomKernel(const Index input_outer_dim_size,",
        "// Line_Reference 110:                                             const Index inner_dim_size,",
        "// Line_Reference 111:                                             const Index output_outer_dim_size,",
        "// Line_Reference 114:   const Index input_total_size = input_outer_dim_size * inner_dim_size;",
        "// Line_Reference 115:   const Index output_total_size = output_outer_dim_size * inner_dim_size;",
        "// Line_Reference 116:   for (int input_index : GpuGridRangeX(input_total_size)) {",
        "// Line_Reference 117:     const Index input_segment_index = input_index / inner_dim_size;",
        "// Line_Reference 118:     const Index segment_offset = input_index % inner_dim_size;",
        "// Line_Reference 120:     if (output_segment_index < 0 || output_segment_index >= output_total_size) {",
        "// Line_Reference 123:     const Index output_index =",
        "// Line_Reference 177:   void operator()(OpKernelContext* ctx, const Index num_segments,",
        "// Line_Reference 178:                   const TensorShape& segment_ids_shape,",
        "// Line_Reference 180:                   const Index data_size, const T* data,",
        "// Line_Reference 199:     const Index input_outer_dim_size = segment_ids.dimension(0);",
        "// Line_Reference 200:     const Index input_inner_dim_size = data_size / input_outer_dim_size;",
        "// Line_Reference 203:     TF_CHECK_OK(",
        "// Line_Reference 204:         GpuLaunchKernel(UnsortedSegmentCustomKernel<T, Index, ReductionF>,",
        "// Line_Reference 205:                         config.block_count, config.thread_per_block, 0,",
        "// Line_Reference 206:                         d.stream(), input_outer_dim_size, input_inner_dim_size,",
        "// Line_Reference 207:                         num_segments, segment_ids.data(), data, output.data()));"
    ]
}
