{
    "cve_id": "CVE-2024-3568",
    "cve_description": "The huggingface/transformers library is vulnerable to arbitrary code execution through deserialization of untrusted data within the `load_repo_checkpoint()` function of the `TFPreTrainedModel()` class. Attackers can execute arbitrary code and commands by crafting a malicious serialized payload, exploiting the use of `pickle.load()` on data from potentially untrusted sources. This vulnerability allows for remote code execution (RCE) by deceiving victims into loading a seemingly harmless checkpoint during a normal training process, thereby enabling attackers to execute arbitrary code on the targeted machine.",
    "cve_publish_date": "2024-04-10",
    "cwe_id": "CWE-502",
    "cwe_name": "Deserialization of Untrusted Data",
    "cwe_description": "The product deserializes untrusted data without sufficiently verifying that the resulting data will be valid.",
    "commit_message": "Remove dead TF loading code (#28926)\n\nRemove dead code",
    "type_of_change": "Modification",
    "filename_of_changes": "modeling_tf_utils.py",
    "code_language": "Python",
    "number_of_lines_added_for_mitigation": "0",
    "number_of_lines_deleted_vulnerable_to_cve": "50",
    "vulnerable_lines": [
        "// Line_Reference 35: from huggingface_hub import Repository, list_repo_files",
        "// Line_Reference 1359:     def load_repo_checkpoint(self, repo_path_or_name):",
        "// Line_Reference 1360:         \"\"\"",
        "// Line_Reference 1361:         Loads a saved checkpoint (model weights and optimizer state) from a repo. Returns the current epoch count when",
        "// Line_Reference 1362:         the checkpoint was made.",
        "// Line_Reference 1363: ",
        "// Line_Reference 1364:         Args:",
        "// Line_Reference 1365:             repo_path_or_name (`str`):",
        "// Line_Reference 1366:                 Can either be a repository name for your {object} in the Hub or a path to a local folder (in which case",
        "// Line_Reference 1367:                 the repository will have the name of that local folder).",
        "// Line_Reference 1368: ",
        "// Line_Reference 1369:         Returns:",
        "// Line_Reference 1370:             `dict`: A dictionary of extra metadata from the checkpoint, most commonly an \"epoch\" count.",
        "// Line_Reference 1371:         \"\"\"",
        "// Line_Reference 1372:         if getattr(self, \"optimizer\", None) is None:",
        "// Line_Reference 1373:             raise RuntimeError(",
        "// Line_Reference 1374:                 \"Checkpoint loading failed as no optimizer is attached to the model. \"",
        "// Line_Reference 1375:                 \"This is most likely caused by the model not being compiled.\"",
        "// Line_Reference 1376:             )",
        "// Line_Reference 1377:         if os.path.isdir(repo_path_or_name):",
        "// Line_Reference 1378:             local_dir = repo_path_or_name",
        "// Line_Reference 1379:         else:",
        "// Line_Reference 1380:             # If this isn't a local path, check that the remote repo exists and has a checkpoint in it",
        "// Line_Reference 1381:             repo_files = list_repo_files(repo_path_or_name)",
        "// Line_Reference 1382:             for file in (\"checkpoint/weights.h5\", \"checkpoint/extra_data.pickle\"):",
        "// Line_Reference 1383:                 if file not in repo_files:",
        "// Line_Reference 1384:                     raise FileNotFoundError(f\"Repo {repo_path_or_name} does not contain checkpoint file {file}!\")",
        "// Line_Reference 1385:             repo = Repository(repo_path_or_name.split(\"/\")[-1], clone_from=repo_path_or_name)",
        "// Line_Reference 1386:             local_dir = repo.local_dir",
        "// Line_Reference 1387: ",
        "// Line_Reference 1388:         # Now make sure the repo actually has a checkpoint in it.",
        "// Line_Reference 1389:         checkpoint_dir = os.path.join(local_dir, \"checkpoint\")",
        "// Line_Reference 1390:         weights_file = os.path.join(checkpoint_dir, \"weights.h5\")",
        "// Line_Reference 1391:         if not os.path.isfile(weights_file):",
        "// Line_Reference 1392:             raise FileNotFoundError(f\"Could not find checkpoint file weights.h5 in repo {repo_path_or_name}!\")",
        "// Line_Reference 1393:         extra_data_file = os.path.join(checkpoint_dir, \"extra_data.pickle\")",
        "// Line_Reference 1394:         if not os.path.isfile(extra_data_file):",
        "// Line_Reference 1395:             raise FileNotFoundError(f\"Could not find checkpoint file extra_data.pickle in repo {repo_path_or_name}!\")",
        "// Line_Reference 1396: ",
        "// Line_Reference 1397:         # Assuming the repo is real and we got a checkpoint, load the weights and the optimizer state into the model.",
        "// Line_Reference 1398:         # The optimizer state includes the iteration count, so learning rate schedules should resume as normal too.",
        "// Line_Reference 1399:         self.load_weights(weights_file)",
        "// Line_Reference 1400:         with open(extra_data_file, \"rb\") as f:",
        "// Line_Reference 1401:             extra_data = pickle.load(f)",
        "// Line_Reference 1402:         self.optimizer.set_weights(extra_data[\"optimizer_state\"])",
        "// Line_Reference 1403: ",
        "// Line_Reference 1404:         # Finally, return the epoch number from the checkpoint. This isn't a property of the model, so we can't",
        "// Line_Reference 1405:         # set it directly, but the user can pass it to fit().",
        "// Line_Reference 1406:         return {\"epoch\": extra_data[\"epoch\"]}",
        "// Line_Reference 1407: "
    ]
}
