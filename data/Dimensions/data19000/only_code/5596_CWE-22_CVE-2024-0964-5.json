from gradio.queueing import Estimation, Event
cancel_tasks,
run_coro_in_background,
set_task_name,
await app.get_blocks()._queue.clean_event(body.event_id)
@app.get("/queue/join", dependencies=[Depends(login_check)])
async def queue_join(
fn_index: int,
session_hash: str,
username: str = Depends(get_current_user),
data: Optional[str] = None,
if blocks._queue.server_app is None:
blocks._queue.set_server_app(app)
event = Event(session_hash, fn_index, request, username)
if data is not None:
input_data = json.loads(data)
event.data = PredictBody(
session_hash=session_hash,
fn_index=fn_index,
data=input_data,
request=request,
)
# Continuous events are not put in the queue so that they do not
# occupy the queue's resource as they are expected to run forever
if blocks.dependencies[event.fn_index].get("every", 0):
await cancel_tasks({f"{event.session_hash}_{event.fn_index}"})
await blocks._queue.reset_iterators(event._id)
blocks._queue.continuous_tasks.append(event)
task = run_coro_in_background(
blocks._queue.process_events, [event], False
)
set_task_name(task, event.session_hash, event.fn_index, batch=False)
app._asyncio_tasks.append(task)
else:
rank = blocks._queue.push(event)
if rank is None:
event.send_message("queue_full", final=True)
else:
estimation = blocks._queue.get_estimation()
await blocks._queue.send_estimation(event, estimation, rank)
await blocks._queue.clean_event(event)
if not event.alive:
message = event.message_queue.get_nowait()
if message is None:  # end of stream marker
return
await blocks._queue.clean_event(event)
@app.post("/queue/data", dependencies=[Depends(login_check)])
async def queue_data(
blocks = app.get_blocks()
blocks._queue.attach_data(body)
