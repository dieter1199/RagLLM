{
    "cve_id": "CVE-2017-18202",
    "cve_description": "The __oom_reap_task_mm function in mm/oom_kill.c in the Linux kernel before 4.14.4 mishandles gather operations, which allows attackers to cause a denial of service (TLB entry leak or use-after-free) or possibly have unspecified other impact by triggering a copy_to_user call within a certain time window.",
    "cve_publish_date": "2018-02-27",
    "cwe_id": "CWE-416",
    "cwe_name": "Use After Free",
    "cwe_description": "The product reuses or references memory after it has been freed. At some point afterward, the memory may be allocated again and saved in another pointer, while the original pointer references a location somewhere within the new allocation. Any operations using the original pointer are no longer valid because the memory \"belongs\" to the code that operates on the new pointer.",
    "commit_message": "mm, oom_reaper: gather each vma to prevent leaking TLB entry\n\ntlb_gather_mmu(&tlb, mm, 0, -1) means gathering the whole virtual memory\nspace.  In this case, tlb->fullmm is true.  Some archs like arm64\ndoesn't flush TLB when tlb->fullmm is true:\n\n  commit 5a7862e83000 (\"arm64: tlbflush: avoid flushing when fullmm == 1\").\n\nWhich causes leaking of tlb entries.\n\nWill clarifies his patch:\n \"Basically, we tag each address space with an ASID (PCID on x86) which\n  is resident in the TLB. This means we can elide TLB invalidation when\n  pulling down a full mm because we won't ever assign that ASID to\n  another mm without doing TLB invalidation elsewhere (which actually\n  just nukes the whole TLB).\n\n  I think that means that we could potentially not fault on a kernel\n  uaccess, because we could hit in the TLB\"\n\nThere could be a window between complete_signal() sending IPI to other\ncores and all threads sharing this mm are really kicked off from cores.\nIn this window, the oom reaper may calls tlb_flush_mmu_tlbonly() to\nflush TLB then frees pages.  However, due to the above problem, the TLB\nentries are not really flushed on arm64.  Other threads are possible to\naccess these pages through TLB entries.  Moreover, a copy_to_user() can\nalso write to these pages without generating page fault, causes\nuse-after-free bugs.\n\nThis patch gathers each vma instead of gathering full vm space.  In this\ncase tlb->fullmm is not true.  The behavior of oom reaper become similar\nto munmapping before do_exit, which should be safe for all archs.\n\nLink: http://lkml.kernel.org/r/20171107095453.179940-1-wangnan0@huawei.com\nFixes: aac453635549 (\"mm, oom: introduce oom reaper\")\nSigned-off-by: Wang Nan <wangnan0@huawei.com>\nAcked-by: Michal Hocko <mhocko@suse.com>\nAcked-by: David Rientjes <rientjes@google.com>\nCc: Minchan Kim <minchan@kernel.org>\nCc: Will Deacon <will.deacon@arm.com>\nCc: Bob Liu <liubo95@huawei.com>\nCc: Ingo Molnar <mingo@kernel.org>\nCc: Roman Gushchin <guro@fb.com>\nCc: Konstantin Khlebnikov <khlebnikov@yandex-team.ru>\nCc: Andrea Arcangeli <aarcange@redhat.com>\nCc: <stable@vger.kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\nSigned-off-by: Linus Torvalds <torvalds@linux-foundation.org>",
    "type_of_change": "Modification",
    "filename_of_changes": "oom_kill.c",
    "code_language": "C",
    "number_of_lines_added_for_mitigation": "4",
    "number_of_lines_deleted_vulnerable_to_cve": "3",
    "vulnerable_lines": [
        "// Line_Reference 553: \ttlb_gather_mmu(&tlb, mm, 0, -1);",
        "// Line_Reference 568: \t\tif (vma_is_anonymous(vma) || !(vma->vm_flags & VM_SHARED))",
        "// Line_Reference 572: \ttlb_finish_mmu(&tlb, 0, -1);"
    ]
}
