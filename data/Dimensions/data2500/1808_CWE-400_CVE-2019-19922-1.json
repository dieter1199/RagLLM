{
    "cve_id": "CVE-2019-19922",
    "cve_description": "kernel/sched/fair.c in the Linux kernel before 5.3.9, when cpu.cfs_quota_us is used (e.g., with Kubernetes), allows attackers to cause a denial of service against non-cpu-bound applications by generating a workload that triggers unwanted slice expiration, aka CID-de53fd7aedb1. (In other words, although this slice expiration would typically be seen with benign workloads, it is possible that an attacker could calculate how many stray requests are required to force an entire Kubernetes cluster into a low-performance state caused by slice expiration, and ensure that a DDoS attack sent that number of stray requests. An attack does not affect the stability of the kernel; it only causes mismanagement of application execution.)",
    "cve_publish_date": "2019-12-22",
    "cwe_id": "CWE-400",
    "cwe_name": "Uncontrolled Resource Consumption",
    "cwe_description": "The product does not properly control the allocation and maintenance of a limited resource, thereby enabling an actor to influence the amount of resources consumed, eventually leading to the exhaustion of available resources.",
    "commit_message": "sched/fair: Fix low cpu usage with high throttling by removing expiration of cpu-local slices\n\nIt has been observed, that highly-threaded, non-cpu-bound applications\nrunning under cpu.cfs_quota_us constraints can hit a high percentage of\nperiods throttled while simultaneously not consuming the allocated\namount of quota. This use case is typical of user-interactive non-cpu\nbound applications, such as those running in kubernetes or mesos when\nrun on multiple cpu cores.\n\nThis has been root caused to cpu-local run queue being allocated per cpu\nbandwidth slices, and then not fully using that slice within the period.\nAt which point the slice and quota expires. This expiration of unused\nslice results in applications not being able to utilize the quota for\nwhich they are allocated.\n\nThe non-expiration of per-cpu slices was recently fixed by\n'commit 512ac999d275 (\"sched/fair: Fix bandwidth timer clock drift\ncondition\")'. Prior to that it appears that this had been broken since\nat least 'commit 51f2176d74ac (\"sched/fair: Fix unlocked reads of some\ncfs_b->quota/period\")' which was introduced in v3.16-rc1 in 2014. That\nadded the following conditional which resulted in slices never being\nexpired.\n\nif (cfs_rq->runtime_expires != cfs_b->runtime_expires) {\n\t/* extend local deadline, drift is bounded above by 2 ticks */\n\tcfs_rq->runtime_expires += TICK_NSEC;\n\nBecause this was broken for nearly 5 years, and has recently been fixed\nand is now being noticed by many users running kubernetes\n(https://github.com/kubernetes/kubernetes/issues/67577) it is my opinion\nthat the mechanisms around expiring runtime should be removed\naltogether.\n\nThis allows quota already allocated to per-cpu run-queues to live longer\nthan the period boundary. This allows threads on runqueues that do not\nuse much CPU to continue to use their remaining slice over a longer\nperiod of time than cpu.cfs_period_us. However, this helps prevent the\nabove condition of hitting throttling while also not fully utilizing\nyour cpu quota.\n\nThis theoretically allows a machine to use slightly more than its\nallotted quota in some periods. This overflow would be bounded by the\nremaining quota left on each per-cpu runqueueu. This is typically no\nmore than min_cfs_rq_runtime=1ms per cpu. For CPU bound tasks this will\nchange nothing, as they should theoretically fully utilize all of their\nquota in each period. For user-interactive tasks as described above this\nprovides a much better user/application experience as their cpu\nutilization will more closely match the amount they requested when they\nhit throttling. This means that cpu limits no longer strictly apply per\nperiod for non-cpu bound applications, but that they are still accurate\nover longer timeframes.\n\nThis greatly improves performance of high-thread-count, non-cpu bound\napplications with low cfs_quota_us allocation on high-core-count\nmachines. In the case of an artificial testcase (10ms/100ms of quota on\n80 CPU machine), this commit resulted in almost 30x performance\nimprovement, while still maintaining correct cpu quota restrictions.\nThat testcase is available at https://github.com/indeedeng/fibtest.\n\nFixes: 512ac999d275 (\"sched/fair: Fix bandwidth timer clock drift condition\")\nSigned-off-by: Dave Chiluk <chiluk+linux@indeed.com>\nSigned-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>\nReviewed-by: Phil Auld <pauld@redhat.com>\nReviewed-by: Ben Segall <bsegall@google.com>\nCc: Ingo Molnar <mingo@redhat.com>\nCc: John Hammond <jhammond@indeed.com>\nCc: Jonathan Corbet <corbet@lwn.net>\nCc: Kyle Anderson <kwa@yelp.com>\nCc: Gabriel Munos <gmunoz@netflix.com>\nCc: Peter Oskolkov <posk@posk.io>\nCc: Cong Wang <xiyou.wangcong@gmail.com>\nCc: Brendan Gregg <bgregg@netflix.com>\nLink: https://lkml.kernel.org/r/1563900266-19734-2-git-send-email-chiluk+linux@indeed.com",
    "type_of_change": "Modification",
    "filename_of_changes": "fair.c",
    "code_language": "C",
    "number_of_lines_added_for_mitigation": "7",
    "number_of_lines_deleted_vulnerable_to_cve": "65",
    "vulnerable_lines": [
        "// Line_Reference 4374: \tcfs_b->runtime_expires = now + ktime_to_ns(cfs_b->period);",
        "// Line_Reference 4375: \tcfs_b->expires_seq++;",
        "// Line_Reference 4397: \tu64 amount = 0, min_amount, expires;",
        "// Line_Reference 4398: \tint expires_seq;",
        "// Line_Reference 4415: \texpires_seq = cfs_b->expires_seq;",
        "// Line_Reference 4416: \texpires = cfs_b->runtime_expires;",
        "// Line_Reference 4420: \t/*",
        "// Line_Reference 4421: \t * we may have advanced our local expiration to account for allowed",
        "// Line_Reference 4422: \t * spread between our sched_clock and the one on which runtime was",
        "// Line_Reference 4423: \t * issued.",
        "// Line_Reference 4424: \t */",
        "// Line_Reference 4425: \tif (cfs_rq->expires_seq != expires_seq) {",
        "// Line_Reference 4426: \t\tcfs_rq->expires_seq = expires_seq;",
        "// Line_Reference 4427: \t\tcfs_rq->runtime_expires = expires;",
        "// Line_Reference 4428: \t}",
        "// Line_Reference 4433: /*",
        "// Line_Reference 4434:  * Note: This depends on the synchronization provided by sched_clock and the",
        "// Line_Reference 4435:  * fact that rq->clock snapshots this value.",
        "// Line_Reference 4436:  */",
        "// Line_Reference 4437: static void expire_cfs_rq_runtime(struct cfs_rq *cfs_rq)",
        "// Line_Reference 4438: {",
        "// Line_Reference 4439: \tstruct cfs_bandwidth *cfs_b = tg_cfs_bandwidth(cfs_rq->tg);",
        "// Line_Reference 4440: ",
        "// Line_Reference 4441: \t/* if the deadline is ahead of our clock, nothing to do */",
        "// Line_Reference 4442: \tif (likely((s64)(rq_clock(rq_of(cfs_rq)) - cfs_rq->runtime_expires) < 0))",
        "// Line_Reference 4443: \t\treturn;",
        "// Line_Reference 4444: ",
        "// Line_Reference 4445: \tif (cfs_rq->runtime_remaining < 0)",
        "// Line_Reference 4446: \t\treturn;",
        "// Line_Reference 4447: ",
        "// Line_Reference 4448: \t/*",
        "// Line_Reference 4449: \t * If the local deadline has passed we have to consider the",
        "// Line_Reference 4450: \t * possibility that our sched_clock is 'fast' and the global deadline",
        "// Line_Reference 4451: \t * has not truly expired.",
        "// Line_Reference 4452: \t *",
        "// Line_Reference 4453: \t * Fortunately we can check determine whether this the case by checking",
        "// Line_Reference 4454: \t * whether the global deadline(cfs_b->expires_seq) has advanced.",
        "// Line_Reference 4455: \t */",
        "// Line_Reference 4456: \tif (cfs_rq->expires_seq == cfs_b->expires_seq) {",
        "// Line_Reference 4457: \t\t/* extend local deadline, drift is bounded above by 2 ticks */",
        "// Line_Reference 4458: \t\tcfs_rq->runtime_expires += TICK_NSEC;",
        "// Line_Reference 4459: \t} else {",
        "// Line_Reference 4460: \t\t/* global deadline is ahead, expiration has passed */",
        "// Line_Reference 4461: \t\tcfs_rq->runtime_remaining = 0;",
        "// Line_Reference 4462: \t}",
        "// Line_Reference 4463: }",
        "// Line_Reference 4464: ",
        "// Line_Reference 4469: \texpire_cfs_rq_runtime(cfs_rq);",
        "// Line_Reference 4664: static u64 distribute_cfs_runtime(struct cfs_bandwidth *cfs_b,",
        "// Line_Reference 4665: \t\tu64 remaining, u64 expires)",
        "// Line_Reference 4687: \t\tcfs_rq->runtime_expires = expires;",
        "// Line_Reference 4712: \tu64 runtime, runtime_expires;",
        "// Line_Reference 4740: \truntime_expires = cfs_b->runtime_expires;",
        "// Line_Reference 4741: ",
        "// Line_Reference 4754: \t\truntime = distribute_cfs_runtime(cfs_b, runtime,",
        "// Line_Reference 4755: \t\t\t\t\t\t runtime_expires);",
        "// Line_Reference 4837: \tif (cfs_b->quota != RUNTIME_INF &&",
        "// Line_Reference 4838: \t    cfs_rq->runtime_expires == cfs_b->runtime_expires) {",
        "// Line_Reference 4871: \tu64 expires;",
        "// Line_Reference 4889: \texpires = cfs_b->runtime_expires;",
        "// Line_Reference 4898: \truntime = distribute_cfs_runtime(cfs_b, runtime, expires);",
        "// Line_Reference 4901: \tif (expires == cfs_b->runtime_expires)",
        "// Line_Reference 4902: \t\tlsub_positive(&cfs_b->runtime, runtime);",
        "// Line_Reference 5059: \tcfs_b->runtime_expires += (overrun + 1) * ktime_to_ns(cfs_b->period);",
        "// Line_Reference 5060: \tcfs_b->expires_seq++;"
    ]
}
