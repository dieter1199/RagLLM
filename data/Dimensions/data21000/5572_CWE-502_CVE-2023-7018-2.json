{
    "cve_id": "CVE-2023-7018",
    "cve_description": "Deserialization of Untrusted Data in GitHub repository huggingface/transformers prior to 4.36.",
    "cve_publish_date": "2023-12-20",
    "cwe_id": "CWE-502",
    "cwe_name": "Deserialization of Untrusted Data",
    "cwe_description": "The product deserializes untrusted data without sufficiently verifying that the resulting data will be valid.",
    "commit_message": "Disallow `pickle.load` unless `TRUST_REMOTE_CODE=True` (#27776)\n\n* fix\r\n\r\n* fix\r\n\r\n* Use TRUST_REMOTE_CODE\r\n\r\n* fix doc\r\n\r\n* fix\r\n\r\n---------\r\n\r\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "type_of_change": "Modification",
    "filename_of_changes": "test_retrieval_rag.py",
    "code_language": "Python",
    "number_of_lines_added_for_mitigation": "0",
    "number_of_lines_deleted_vulnerable_to_cve": "59",
    "vulnerable_lines": [
        "// Line_Reference 17: import pickle",
        "// Line_Reference 177:     def get_dummy_legacy_index_retriever(self):",
        "// Line_Reference 178:         dataset = Dataset.from_dict(",
        "// Line_Reference 179:             {",
        "// Line_Reference 180:                 \"id\": [\"0\", \"1\"],",
        "// Line_Reference 181:                 \"text\": [\"foo\", \"bar\"],",
        "// Line_Reference 182:                 \"title\": [\"Foo\", \"Bar\"],",
        "// Line_Reference 183:                 \"embeddings\": [np.ones(self.retrieval_vector_size + 1), 2 * np.ones(self.retrieval_vector_size + 1)],",
        "// Line_Reference 184:             }",
        "// Line_Reference 185:         )",
        "// Line_Reference 186:         dataset.add_faiss_index(\"embeddings\", string_factory=\"Flat\", metric_type=faiss.METRIC_INNER_PRODUCT)",
        "// Line_Reference 187: ",
        "// Line_Reference 188:         index_file_name = os.path.join(self.tmpdirname, \"hf_bert_base.hnswSQ8_correct_phi_128.c_index\")",
        "// Line_Reference 189:         dataset.save_faiss_index(\"embeddings\", index_file_name + \".index.dpr\")",
        "// Line_Reference 190:         pickle.dump(dataset[\"id\"], open(index_file_name + \".index_meta.dpr\", \"wb\"))",
        "// Line_Reference 191: ",
        "// Line_Reference 192:         passages_file_name = os.path.join(self.tmpdirname, \"psgs_w100.tsv.pkl\")",
        "// Line_Reference 193:         passages = {sample[\"id\"]: [sample[\"text\"], sample[\"title\"]] for sample in dataset}",
        "// Line_Reference 194:         pickle.dump(passages, open(passages_file_name, \"wb\"))",
        "// Line_Reference 195: ",
        "// Line_Reference 196:         config = RagConfig(",
        "// Line_Reference 197:             retrieval_vector_size=self.retrieval_vector_size,",
        "// Line_Reference 198:             question_encoder=DPRConfig().to_dict(),",
        "// Line_Reference 199:             generator=BartConfig().to_dict(),",
        "// Line_Reference 200:             index_name=\"legacy\",",
        "// Line_Reference 201:             index_path=self.tmpdirname,",
        "// Line_Reference 202:         )",
        "// Line_Reference 203:         retriever = RagRetriever(",
        "// Line_Reference 204:             config, question_encoder_tokenizer=self.get_dpr_tokenizer(), generator_tokenizer=self.get_bart_tokenizer()",
        "// Line_Reference 205:         )",
        "// Line_Reference 206:         return retriever",
        "// Line_Reference 207: ",
        "// Line_Reference 291:     def test_legacy_index_retriever_retrieve(self):",
        "// Line_Reference 292:         n_docs = 1",
        "// Line_Reference 293:         retriever = self.get_dummy_legacy_index_retriever()",
        "// Line_Reference 294:         hidden_states = np.array(",
        "// Line_Reference 295:             [np.ones(self.retrieval_vector_size), -np.ones(self.retrieval_vector_size)], dtype=np.float32",
        "// Line_Reference 296:         )",
        "// Line_Reference 297:         retrieved_doc_embeds, doc_ids, doc_dicts = retriever.retrieve(hidden_states, n_docs=n_docs)",
        "// Line_Reference 298:         self.assertEqual(retrieved_doc_embeds.shape, (2, n_docs, self.retrieval_vector_size))",
        "// Line_Reference 299:         self.assertEqual(len(doc_dicts), 2)",
        "// Line_Reference 300:         self.assertEqual(sorted(doc_dicts[0]), [\"text\", \"title\"])",
        "// Line_Reference 301:         self.assertEqual(len(doc_dicts[0][\"text\"]), n_docs)",
        "// Line_Reference 302:         self.assertEqual(doc_dicts[0][\"text\"][0], \"bar\")  # max inner product is reached with second doc",
        "// Line_Reference 303:         self.assertEqual(doc_dicts[1][\"text\"][0], \"foo\")  # max inner product is reached with first doc",
        "// Line_Reference 304:         self.assertListEqual(doc_ids.tolist(), [[1], [0]])",
        "// Line_Reference 305: ",
        "// Line_Reference 306:     def test_legacy_hf_index_retriever_save_and_from_pretrained(self):",
        "// Line_Reference 307:         retriever = self.get_dummy_legacy_index_retriever()",
        "// Line_Reference 308:         with tempfile.TemporaryDirectory() as tmp_dirname:",
        "// Line_Reference 309:             retriever.save_pretrained(tmp_dirname)",
        "// Line_Reference 310:             retriever = RagRetriever.from_pretrained(tmp_dirname)",
        "// Line_Reference 311:             self.assertIsInstance(retriever, RagRetriever)",
        "// Line_Reference 312:             hidden_states = np.array(",
        "// Line_Reference 313:                 [np.ones(self.retrieval_vector_size), -np.ones(self.retrieval_vector_size)], dtype=np.float32",
        "// Line_Reference 314:             )",
        "// Line_Reference 315:             out = retriever.retrieve(hidden_states, n_docs=1)",
        "// Line_Reference 316:             self.assertTrue(out is not None)",
        "// Line_Reference 317: "
    ]
}
