static void io_prep_async_work(struct io_kiocb *req)
io_req_init_async(req);
if (req->flags & REQ_F_ISREG) {
if (def->hash_reg_file || (ctx->flags & IORING_SETUP_IOPOLL))
io_wq_hash_work(&req->work, file_inode(req->file));
} else {
if (def->unbound_nonreg_file)
req->work.flags |= IO_WQ_WORK_UNBOUND;
}
(io_op_defs[req->opcode].work_flags & IO_WQ_WORK_FILES) &&
req->work.identity->files = get_files_struct(current);
get_nsproxy(current->nsproxy);
req->work.identity->nsproxy = current->nsproxy;
if (!(req->work.flags & IO_WQ_WORK_MM) &&
(def->work_flags & IO_WQ_WORK_MM)) {
mmgrab(current->mm);
req->work.identity->mm = current->mm;
req->work.flags |= IO_WQ_WORK_MM;
}
req->work.identity->blkcg_css = blkcg_css();
if (css_tryget_online(req->work.identity->blkcg_css))
req->work.identity->creds = get_current_cred();
spin_lock(&current->fs->lock);
if (!current->fs->in_exec) {
req->work.identity->fs = current->fs;
req->work.identity->fs->users++;
if (def->needs_fsize)
req->work.identity->fsize = rlimit(RLIMIT_FSIZE);
else
req->work.identity->fsize = RLIM_INFINITY;
struct io_uring_task *tctx;
struct io_ring_ctx *ctx;
tctx = req->task->io_uring;
ctx = req->ctx;
req->work.identity->creds = idr_find(&ctx->personality_idr, id);
if (unlikely(!req->work.identity->creds))
get_cred(req->work.identity->creds);
const struct cred *cred;
cred = idr_remove(&ctx->personality_idr, id);
if (cred)
put_cred(cred);
const struct cred *creds = get_current_cred();
int id;
id = idr_alloc_cyclic(&ctx->personality_idr, (void *) creds, 1,
USHRT_MAX, GFP_KERNEL);
if (id < 0)
put_cred(creds);
return id;
const struct cred *old_creds;
old_creds = idr_remove(&ctx->personality_idr, id);
if (old_creds) {
put_cred(old_creds);