__global__ void UnsortedSegmentCustomKernel(const Index input_outer_dim_size,
const Index inner_dim_size,
const Index output_outer_dim_size,
const Index input_total_size = input_outer_dim_size * inner_dim_size;
const Index output_total_size = output_outer_dim_size * inner_dim_size;
for (int input_index : GpuGridRangeX(input_total_size)) {
const Index input_segment_index = input_index / inner_dim_size;
const Index segment_offset = input_index % inner_dim_size;
if (output_segment_index < 0 || output_segment_index >= output_total_size) {
const Index output_index =
void operator()(OpKernelContext* ctx, const Index num_segments,
const TensorShape& segment_ids_shape,
const Index data_size, const T* data,
const Index input_outer_dim_size = segment_ids.dimension(0);
const Index input_inner_dim_size = data_size / input_outer_dim_size;
TF_CHECK_OK(
GpuLaunchKernel(UnsortedSegmentCustomKernel<T, Index, ReductionF>,
config.block_count, config.thread_per_block, 0,
d.stream(), input_outer_dim_size, input_inner_dim_size,
num_segments, segment_ids.data(), data, output.data()));