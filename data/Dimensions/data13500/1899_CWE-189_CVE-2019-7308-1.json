{
    "cve_id": "CVE-2019-7308",
    "cve_description": "kernel/bpf/verifier.c in the Linux kernel before 4.20.6 performs undesirable out-of-bounds speculation on pointer arithmetic in various cases, including cases of different branches with different state or limits to sanitize, leading to side-channel attacks.",
    "cve_publish_date": "2019-02-01",
    "cwe_id": "CWE-189",
    "cwe_name": "Numeric Errors",
    "cwe_description": "Weaknesses in this category are related to improper calculation or conversion of numbers.",
    "commit_message": "bpf: prevent out of bounds speculation on pointer arithmetic\n\nJann reported that the original commit back in b2157399cc98\n(\"bpf: prevent out-of-bounds speculation\") was not sufficient\nto stop CPU from speculating out of bounds memory access:\nWhile b2157399cc98 only focussed on masking array map access\nfor unprivileged users for tail calls and data access such\nthat the user provided index gets sanitized from BPF program\nand syscall side, there is still a more generic form affected\nfrom BPF programs that applies to most maps that hold user\ndata in relation to dynamic map access when dealing with\nunknown scalars or \"slow\" known scalars as access offset, for\nexample:\n\n  - Load a map value pointer into R6\n  - Load an index into R7\n  - Do a slow computation (e.g. with a memory dependency) that\n    loads a limit into R8 (e.g. load the limit from a map for\n    high latency, then mask it to make the verifier happy)\n  - Exit if R7 >= R8 (mispredicted branch)\n  - Load R0 = R6[R7]\n  - Load R0 = R6[R0]\n\nFor unknown scalars there are two options in the BPF verifier\nwhere we could derive knowledge from in order to guarantee\nsafe access to the memory: i) While </>/<=/>= variants won't\nallow to derive any lower or upper bounds from the unknown\nscalar where it would be safe to add it to the map value\npointer, it is possible through ==/!= test however. ii) another\noption is to transform the unknown scalar into a known scalar,\nfor example, through ALU ops combination such as R &= <imm>\nfollowed by R |= <imm> or any similar combination where the\noriginal information from the unknown scalar would be destroyed\nentirely leaving R with a constant. The initial slow load still\nprecedes the latter ALU ops on that register, so the CPU\nexecutes speculatively from that point. Once we have the known\nscalar, any compare operation would work then. A third option\nonly involving registers with known scalars could be crafted\nas described in [0] where a CPU port (e.g. Slow Int unit)\nwould be filled with many dependent computations such that\nthe subsequent condition depending on its outcome has to wait\nfor evaluation on its execution port and thereby executing\nspeculatively if the speculated code can be scheduled on a\ndifferent execution port, or any other form of mistraining\nas described in [1], for example. Given this is not limited\nto only unknown scalars, not only map but also stack access\nis affected since both is accessible for unprivileged users\nand could potentially be used for out of bounds access under\nspeculation.\n\nIn order to prevent any of these cases, the verifier is now\nsanitizing pointer arithmetic on the offset such that any\nout of bounds speculation would be masked in a way where the\npointer arithmetic result in the destination register will\nstay unchanged, meaning offset masked into zero similar as\nin array_index_nospec() case. With regards to implementation,\nthere are three options that were considered: i) new insn\nfor sanitation, ii) push/pop insn and sanitation as inlined\nBPF, iii) reuse of ax register and sanitation as inlined BPF.\n\nOption i) has the downside that we end up using from reserved\nbits in the opcode space, but also that we would require\neach JIT to emit masking as native arch opcodes meaning\nmitigation would have slow adoption till everyone implements\nit eventually which is counter-productive. Option ii) and iii)\nhave both in common that a temporary register is needed in\norder to implement the sanitation as inlined BPF since we\nare not allowed to modify the source register. While a push /\npop insn in ii) would be useful to have in any case, it\nrequires once again that every JIT needs to implement it\nfirst. While possible, amount of changes needed would also\nbe unsuitable for a -stable patch. Therefore, the path which\nhas fewer changes, less BPF instructions for the mitigation\nand does not require anything to be changed in the JITs is\noption iii) which this work is pursuing. The ax register is\nalready mapped to a register in all JITs (modulo arm32 where\nit's mapped to stack as various other BPF registers there)\nand used in constant blinding for JITs-only so far. It can\nbe reused for verifier rewrites under certain constraints.\nThe interpreter's tmp \"register\" has therefore been remapped\ninto extending the register set with hidden ax register and\nreusing that for a number of instructions that needed the\nprior temporary variable internally (e.g. div, mod). This\nallows for zero increase in stack space usage in the interpreter,\nand enables (restricted) generic use in rewrites otherwise as\nlong as such a patchlet does not make use of these instructions.\nThe sanitation mask is dynamic and relative to the offset the\nmap value or stack pointer currently holds.\n\nThere are various cases that need to be taken under consideration\nfor the masking, e.g. such operation could look as follows:\nptr += val or val += ptr or ptr -= val. Thus, the value to be\nsanitized could reside either in source or in destination\nregister, and the limit is different depending on whether\nthe ALU op is addition or subtraction and depending on the\ncurrent known and bounded offset. The limit is derived as\nfollows: limit := max_value_size - (smin_value + off). For\nsubtraction: limit := umax_value + off. This holds because\nwe do not allow any pointer arithmetic that would\ntemporarily go out of bounds or would have an unknown\nvalue with mixed signed bounds where it is unclear at\nverification time whether the actual runtime value would\nbe either negative or positive. For example, we have a\nderived map pointer value with constant offset and bounded\none, so limit based on smin_value works because the verifier\nrequires that statically analyzed arithmetic on the pointer\nmust be in bounds, and thus it checks if resulting\nsmin_value + off and umax_value + off is still within map\nvalue bounds at time of arithmetic in addition to time of\naccess. Similarly, for the case of stack access we derive\nthe limit as follows: MAX_BPF_STACK + off for subtraction\nand -off for the case of addition where off := ptr_reg->off +\nptr_reg->var_off.value. Subtraction is a special case for\nthe masking which can be in form of ptr += -val, ptr -= -val,\nor ptr -= val. In the first two cases where we know that\nthe value is negative, we need to temporarily negate the\nvalue in order to do the sanitation on a positive value\nwhere we later swap the ALU op, and restore original source\nregister if the value was in source.\n\nThe sanitation of pointer arithmetic alone is still not fully\nsufficient as is, since a scenario like the following could\nhappen ...\n\n  PTR += 0x1000 (e.g. K-based imm)\n  PTR -= BIG_NUMBER_WITH_SLOW_COMPARISON\n  PTR += 0x1000\n  PTR -= BIG_NUMBER_WITH_SLOW_COMPARISON\n  [...]\n\n... which under speculation could end up as ...\n\n  PTR += 0x1000\n  PTR -= 0 [ truncated by mitigation ]\n  PTR += 0x1000\n  PTR -= 0 [ truncated by mitigation ]\n  [...]\n\n... and therefore still access out of bounds. To prevent such\ncase, the verifier is also analyzing safety for potential out\nof bounds access under speculative execution. Meaning, it is\nalso simulating pointer access under truncation. We therefore\n\"branch off\" and push the current verification state after the\nALU operation with known 0 to the verification stack for later\nanalysis. Given the current path analysis succeeded it is\nlikely that the one under speculation can be pruned. In any\ncase, it is also subject to existing complexity limits and\ntherefore anything beyond this point will be rejected. In\nterms of pruning, it needs to be ensured that the verification\nstate from speculative execution simulation must never prune\na non-speculative execution path, therefore, we mark verifier\nstate accordingly at the time of push_stack(). If verifier\ndetects out of bounds access under speculative execution from\none of the possible paths that includes a truncation, it will\nreject such program.\n\nGiven we mask every reg-based pointer arithmetic for\nunprivileged programs, we've been looking into how it could\naffect real-world programs in terms of size increase. As the\nmajority of programs are targeted for privileged-only use\ncase, we've unconditionally enabled masking (with its alu\nrestrictions on top of it) for privileged programs for the\nsake of testing in order to check i) whether they get rejected\nin its current form, and ii) by how much the number of\ninstructions and size will increase. We've tested this by\nusing Katran, Cilium and test_l4lb from the kernel selftests.\nFor Katran we've evaluated balancer_kern.o, Cilium bpf_lxc.o\nand an older test object bpf_lxc_opt_-DUNKNOWN.o and l4lb\nwe've used test_l4lb.o as well as test_l4lb_noinline.o. We\nfound that none of the programs got rejected by the verifier\nwith this change, and that impact is rather minimal to none.\nbalancer_kern.o had 13,904 bytes (1,738 insns) xlated and\n7,797 bytes JITed before and after the change. Most complex\nprogram in bpf_lxc.o had 30,544 bytes (3,817 insns) xlated\nand 18,538 bytes JITed before and after and none of the other\ntail call programs in bpf_lxc.o had any changes either. For\nthe older bpf_lxc_opt_-DUNKNOWN.o object we found a small\nincrease from 20,616 bytes (2,576 insns) and 12,536 bytes JITed\nbefore to 20,664 bytes (2,582 insns) and 12,558 bytes JITed\nafter the change. Other programs from that object file had\nsimilar small increase. Both test_l4lb.o had no change and\nremained at 6,544 bytes (817 insns) xlated and 3,401 bytes\nJITed and for test_l4lb_noinline.o constant at 5,080 bytes\n(634 insns) xlated and 3,313 bytes JITed. This can be explained\nin that LLVM typically optimizes stack based pointer arithmetic\nby using K-based operations and that use of dynamic map access\nis not overly frequent. However, in future we may decide to\noptimize the algorithm further under known guarantees from\nbranch and value speculation. Latter seems also unclear in\nterms of prediction heuristics that today's CPUs apply as well\nas whether there could be collisions in e.g. the predictor's\nValue History/Pattern Table for triggering out of bounds access,\nthus masking is performed unconditionally at this point but could\nbe subject to relaxation later on. We were generally also\nbrainstorming various other approaches for mitigation, but the\nblocker was always lack of available registers at runtime and/or\noverhead for runtime tracking of limits belonging to a specific\npointer. Thus, we found this to be minimally intrusive under\ngiven constraints.\n\nWith that in place, a simple example with sanitized access on\nunprivileged load at post-verification time looks as follows:\n\n  # bpftool prog dump xlated id 282\n  [...]\n  28: (79) r1 = *(u64 *)(r7 +0)\n  29: (79) r2 = *(u64 *)(r7 +8)\n  30: (57) r1 &= 15\n  31: (79) r3 = *(u64 *)(r0 +4608)\n  32: (57) r3 &= 1\n  33: (47) r3 |= 1\n  34: (2d) if r2 > r3 goto pc+19\n  35: (b4) (u32) r11 = (u32) 20479  |\n  36: (1f) r11 -= r2                | Dynamic sanitation for pointer\n  37: (4f) r11 |= r2                | arithmetic with registers\n  38: (87) r11 = -r11               | containing bounded or known\n  39: (c7) r11 s>>= 63              | scalars in order to prevent\n  40: (5f) r11 &= r2                | out of bounds speculation.\n  41: (0f) r4 += r11                |\n  42: (71) r4 = *(u8 *)(r4 +0)\n  43: (6f) r4 <<= r1\n  [...]\n\nFor the case where the scalar sits in the destination register\nas opposed to the source register, the following code is emitted\nfor the above example:\n\n  [...]\n  16: (b4) (u32) r11 = (u32) 20479\n  17: (1f) r11 -= r2\n  18: (4f) r11 |= r2\n  19: (87) r11 = -r11\n  20: (c7) r11 s>>= 63\n  21: (5f) r2 &= r11\n  22: (0f) r2 += r0\n  23: (61) r0 = *(u32 *)(r2 +0)\n  [...]\n\nJIT blinding example with non-conflicting use of r10:\n\n  [...]\n   d5:\tje     0x0000000000000106    _\n   d7:\tmov    0x0(%rax),%edi       |\n   da:\tmov    $0xf153246,%r10d     | Index load from map value and\n   e0:\txor    $0xf153259,%r10      | (const blinded) mask with 0x1f.\n   e7:\tand    %r10,%rdi            |_\n   ea:\tmov    $0x2f,%r10d          |\n   f0:\tsub    %rdi,%r10            | Sanitized addition. Both use r10\n   f3:\tor     %rdi,%r10            | but do not interfere with each\n   f6:\tneg    %r10                 | other. (Neither do these instructions\n   f9:\tsar    $0x3f,%r10           | interfere with the use of ax as temp\n   fd:\tand    %r10,%rdi            | in interpreter.)\n  100:\tadd    %rax,%rdi            |_\n  103:\tmov    0x0(%rdi),%eax\n [...]\n\nTested that it fixes Jann's reproducer, and also checked that test_verifier\nand test_progs suite with interpreter, JIT and JIT with hardening enabled\non x86-64 and arm64 runs successfully.\n\n  [0] Speculose: Analyzing the Security Implications of Speculative\n      Execution in CPUs, Giorgi Maisuradze and Christian Rossow,\n      https://arxiv.org/pdf/1801.04084.pdf\n\n  [1] A Systematic Evaluation of Transient Execution Attacks and\n      Defenses, Claudio Canella, Jo Van Bulck, Michael Schwarz,\n      Moritz Lipp, Benjamin von Berg, Philipp Ortner, Frank Piessens,\n      Dmitry Evtyushkin, Daniel Gruss,\n      https://arxiv.org/pdf/1811.05441.pdf\n\nFixes: b2157399cc98 (\"bpf: prevent out-of-bounds speculation\")\nReported-by: Jann Horn <jannh@google.com>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nAcked-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
    "type_of_change": "Modification",
    "filename_of_changes": "verifier.c",
    "code_language": "C",
    "number_of_lines_added_for_mitigation": "179",
    "number_of_lines_deleted_vulnerable_to_cve": "6",
    "vulnerable_lines": [
        "// Line_Reference 757: \t\t\t\t\t     int insn_idx, int prev_insn_idx)",
        "// Line_Reference 4392: \tother_branch = push_stack(env, *insn_idx + insn->off + 1, *insn_idx);",
        "// Line_Reference 5742: \t\t\t\t\tverbose(env, \"\\nfrom %d to %d: safe\\n\",",
        "// Line_Reference 5743: \t\t\t\t\t\tenv->prev_insn_idx, env->insn_idx);",
        "// Line_Reference 5760: \t\t\t\tverbose(env, \"\\nfrom %d to %d:\",",
        "// Line_Reference 5761: \t\t\t\t\tenv->prev_insn_idx, env->insn_idx);"
    ]
}
