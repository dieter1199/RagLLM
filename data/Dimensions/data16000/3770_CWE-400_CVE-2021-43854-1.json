{
    "cve_id": "CVE-2021-43854",
    "cve_description": "NLTK (Natural Language Toolkit) is a suite of open source Python modules, data sets, and tutorials supporting research and development in Natural Language Processing. Versions prior to 3.6.5 are vulnerable to regular expression denial of service (ReDoS) attacks. The vulnerability is present in PunktSentenceTokenizer, sent_tokenize and word_tokenize. Any users of this class, or these two functions, are vulnerable to the ReDoS attack. In short, a specifically crafted long input to any of these vulnerable functions will cause them to take a significant amount of execution time. If your program relies on any of the vulnerable functions for tokenizing unpredictable user input, then we would strongly recommend upgrading to a version of NLTK without the vulnerability. For users unable to upgrade the execution time can be bounded by limiting the maximum length of an input to any of the vulnerable functions. Our recommendation is to implement such a limit.",
    "cve_publish_date": "2021-12-23",
    "cwe_id": "CWE-400",
    "cwe_name": "Uncontrolled Resource Consumption",
    "cwe_description": "The product does not properly control the allocation and maintenance of a limited resource, thereby enabling an actor to influence the amount of resources consumed, eventually leading to the exhaustion of available resources.",
    "commit_message": "Resolved serious ReDoS in PunktSentenceTokenizer (#2869)\n\n* Resolved serious ReDOS in PunktSentenceTokenizer\r\n\r\n* Improve performance by relying on string split instead of re.search\r\n\r\n* Solved issue if sentence contains just one token",
    "type_of_change": "Modification",
    "filename_of_changes": "punkt.py",
    "code_language": "Python",
    "number_of_lines_added_for_mitigation": "61",
    "number_of_lines_deleted_vulnerable_to_cve": "5",
    "vulnerable_lines": [
        "// Line_Reference 269:         \\S*                          # some word material",
        "// Line_Reference 1287:         for match in self._lang_vars.period_context_re().finditer(text):",
        "// Line_Reference 1288:             decision_text = match.group() + match.group(\"after_tok\")",
        "// Line_Reference 1338:         for match in self._lang_vars.period_context_re().finditer(text):",
        "// Line_Reference 1339:             context = match.group() + match.group(\"after_tok\")"
    ]
}
