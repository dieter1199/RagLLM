{
    "cve_id": "CVE-2023-22735",
    "cve_description": "Zulip is an open-source team collaboration tool. In versions of zulip prior to commit `2f6c5a8` but after commit `04cf68b` users could upload files with arbitrary `Content-Type` which would be served from the Zulip hostname with `Content-Disposition: inline` and no `Content-Security-Policy` header, allowing them to trick other users into executing arbitrary Javascript in the context of the Zulip application.  Among other things, this enables session theft. Only deployments which use the S3 storage (not the local-disk storage) are affected, and only deployments which deployed commit 04cf68b45ebb5c03247a0d6453e35ffc175d55da, which has only been in `main`, not any numbered release. Users affected should upgrade from main again to deploy this fix. Switching from S3 storage to the local-disk storage would nominally mitigate this, but is likely more involved than upgrading to the latest `main` which addresses the issue.",
    "cve_publish_date": "2023-02-07",
    "cwe_id": "CWE-436",
    "cwe_name": "Interpretation Conflict",
    "cwe_description": "Product A handles inputs or steps differently than Product B, which causes A to perform incorrect actions based on its perception of B's state.",
    "commit_message": "uploads: Serve S3 uploads directly from nginx.\n\nWhen file uploads are stored in S3, this means that Zulip serves as a\n302 to S3.  Because browsers do not cache redirects, this means that\nno image contents can be cached -- and upon every page load or reload,\nevery recently-posted image must be re-fetched.  This incurs extra\nload on the Zulip server, as well as potentially excessive bandwidth\nusage from S3, and on the client's connection.\n\nSwitch to fetching the content from S3 in nginx, and serving the\ncontent from nginx.  These have `Cache-control: private, immutable`\nheaders set on the response, allowing browsers to cache them locally.\n\nBecause nginx fetching from S3 can be slow, and requests for uploads\nwill generally be bunched around when a message containing them are\nfirst posted, we instruct nginx to cache the contents locally.  This\nis safe because uploaded file contents are immutable; access control\nis still mediated by Django.  The nginx cache key is the URL without\nquery parameters, as those parameters include a time-limited signed\nauthentication parameter which lets nginx fetch the non-public file.\n\nThis adds a number of nginx-level configuration parameters to control\nthe caching which nginx performs, including the amount of in-memory\nindex for he cache, the maximum storage of the cache on disk, and how\nlong data is retained in the cache.  The currently-chosen figures are\nreasonable for small to medium deployments.\n\nThe most notable effect of this change is in allowing browsers to\ncache uploaded image content; however, while there will be many fewer\nrequests, it also has an improvement on request latency.  The\nfollowing tests were done with a non-AWS client in SFO, a server and\nS3 storage in us-east-1, and with 100 requests after 10 requests of\nwarm-up (to fill the nginx cache).  The mean and standard deviation\nare shown.\n\n|                   | Redirect to S3      | Caching proxy, hot  | Caching proxy, cold |\n| ----------------- | ------------------- | ------------------- | ------------------- |\n| Time in Django    | 263.0 ms ±  28.3 ms | 258.0 ms ±  12.3 ms | 258.0 ms ±  12.3 ms |\n| Small file (842b) | 586.1 ms ±  21.1 ms | 266.1 ms ±  67.4 ms | 288.6 ms ±  17.7 ms |\n| Large file (660k) | 959.6 ms ± 137.9 ms | 609.5 ms ±  13.0 ms | 648.1 ms ±  43.2 ms |\n\nThe hot-cache performance is faster for both large and small files,\nsince it saves the client the time having to make a second request to\na separate host.  This performance improvement remains at least 100ms\neven if the client is on the same coast as the server.\n\nCold nginx caches are only slightly slower than hot caches, because\nVPC access to S3 endpoints is extremely fast (assuming it is in the\nsame region as the host), and nginx can pool connections to S3 and\nreuse them.\n\nHowever, all of the 648ms taken to serve a cold-cache large file is\noccupied in nginx, as opposed to the only 263ms which was spent in\nnginx when using redirects to S3.  This means that to overall spend\nless time responding to uploaded-file requests in nginx, clients will\nneed to find files in their local cache, and skip making an\nuploaded-file request, at least 60% of the time.  Modeling shows a\nreduction in the number of client requests by about 70% - 80%.\n\nThe `Content-Disposition` header logic can now also be entirely shared\nwith the local-file codepath, as can the `url_only` path used by\nmobile clients.  While we could provide the direct-to-S3 temporary\nsigned URL to mobile clients, we choose to provide the\nserved-from-Zulip signed URL, to better control caching headers on it,\nand greater consistency.  In doing so, we adjust the salt used for the\nURL; since these URLs are only valid for 60s, the effect of this salt\nchange is minimal.",
    "type_of_change": "Modification",
    "filename_of_changes": "urls.py",
    "code_language": "Python",
    "number_of_lines_added_for_mitigation": "3",
    "number_of_lines_deleted_vulnerable_to_cve": "3",
    "vulnerable_lines": [
        "// Line_Reference 170:     serve_local_file_unauthed,",
        "// Line_Reference 642:         serve_local_file_unauthed,",
        "// Line_Reference 643:         name=\"local_file_unauthed\","
    ]
}
