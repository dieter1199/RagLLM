{
    "cve_id": "CVE-2020-15209",
    "cve_description": "In tensorflow-lite before versions 1.15.4, 2.0.3, 2.1.2, 2.2.1 and 2.3.1, a crafted TFLite model can force a node to have as input a tensor backed by a `nullptr` buffer. This can be achieved by changing a buffer index in the flatbuffer serialization to convert a read-only tensor to a read-write one. The runtime assumes that these buffers are written to before a possible read, hence they are initialized with `nullptr`. However, by changing the buffer index for a tensor and implicitly converting that tensor to be a read-write one, as there is nothing in the model that writes to it, we get a null pointer dereference. The issue is patched in commit 0b5662bc, and is released in TensorFlow versions 1.15.4, 2.0.3, 2.1.2, 2.2.1, or 2.3.1.",
    "cve_publish_date": "2020-09-25",
    "cwe_id": "CWE-476",
    "cwe_name": "NULL Pointer Dereference",
    "cwe_description": "The product dereferences a pointer that it expects to be valid but is NULL.",
    "commit_message": "[tflite] Ensure input tensors don't have `nullptr` buffers.\n\nA crafted TFLite model can force a node to have as input a tensor backed by a `nullptr` buffer. That is, by carefully changing the buffer index in the flatbuffer serialization, we can force the TFLite interpreter to consider a read-only tensor to be a read-write one and assume that there is an operator that has this tensor as output, writing to it and allocating memory before the tensor is used as input. If this does not happen, we get memory corruption.\n\nPiperOrigin-RevId: 332524692\nChange-Id: I57ef175152a29020af9ab041dc959e5631dce40f",
    "type_of_change": "Modification",
    "filename_of_changes": "model_test.cc",
    "code_language": "C++",
    "number_of_lines_added_for_mitigation": "42",
    "number_of_lines_deleted_vulnerable_to_cve": "18",
    "vulnerable_lines": [
        "// Line_Reference 441: TEST(BasicFlatBufferModel, TestHandleMalformedModel) {",
        "// Line_Reference 442:   const auto model_paths = {",
        "// Line_Reference 443:       // These models use the same tensor as both input and ouput of a node",
        "// Line_Reference 444:       \"tensorflow/lite/testdata/add_shared_tensors.bin\",",
        "// Line_Reference 445:   };",
        "// Line_Reference 446: ",
        "// Line_Reference 447:   for (const auto& model_path : model_paths) {",
        "// Line_Reference 448:     std::unique_ptr<tflite::FlatBufferModel> model =",
        "// Line_Reference 449:         FlatBufferModel::BuildFromFile(model_path);",
        "// Line_Reference 450:     ASSERT_NE(model, nullptr);",
        "// Line_Reference 451: ",
        "// Line_Reference 452:     tflite::ops::builtin::BuiltinOpResolver resolver;",
        "// Line_Reference 453:     InterpreterBuilder builder(*model, resolver);",
        "// Line_Reference 454:     std::unique_ptr<Interpreter> interpreter;",
        "// Line_Reference 455:     ASSERT_EQ(builder(&interpreter), kTfLiteOk);",
        "// Line_Reference 456:     ASSERT_NE(interpreter, nullptr);",
        "// Line_Reference 457:     ASSERT_NE(interpreter->AllocateTensors(), kTfLiteOk);",
        "// Line_Reference 458:   }"
    ]
}
