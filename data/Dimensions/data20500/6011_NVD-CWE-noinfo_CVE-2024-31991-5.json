{
    "cve_id": "CVE-2024-31991",
    "cve_description": "Mealie is a self hosted recipe manager and meal planner. Prior to 1.4.0, the safe_scrape_html function utilizes a user-controlled URL to issue a request to a remote server. Based on the content of the response, it will either parse the content or disregard it. This function, nor those that call it, add any restrictions on the URL that can be provided, nor is it restricted to being an FQDN (i.e., an IP address can be provided). As this functionâ€™s return will be handled differently by its caller depending on the response, it is possible for an attacker to use this functionality to positively identify HTTP(s) servers on the local network with any IP/port combination. This issue can result in any authenticated user being able to map HTTP servers on a local network that the Mealie service has access to. Note that by default any user can create an account on a Mealie server, and that the default changeme@example.com user is available with its hard-coded password. This vulnerability is fixed in 1.4.0.",
    "cve_publish_date": "2024-04-19",
    "cwe_id": "NVD-CWE-noinfo",
    "cwe_name": "Insufficient Information",
    "cwe_description": "There is insufficient information about the issue to classify it; details are unkown or unspecified.",
    "commit_message": "security: gh security recs (#3368)\n\n* change ALLOW_SIGNUP to default to false\r\n\r\n* add 1.4.0 tag for OIDC docs\r\n\r\n* new notes on security inline with security/policy review\r\n\r\n* safer transport for external requests\r\n\r\n* fix linter errors\r\n\r\n* docs: Tidy up wording/formatting\r\n\r\n* fix request errors\r\n\r\n* whoops\r\n\r\n* fix implementation with std lib\r\n\r\n* format\r\n\r\n* Remove check on netloc_parts. It only includes URL after any @\r\n\r\n---------\r\n\r\nCo-authored-by: boc-the-git <3479092+boc-the-git@users.noreply.github.com>\r\nCo-authored-by: Brendan <b.oconnell14@gmail.com>",
    "type_of_change": "ModificationType.ADD",
    "filename_of_changes": "recipe_data_service.py",
    "code_language": "Python",
    "number_of_lines_added_for_mitigation": "17",
    "number_of_lines_deleted_vulnerable_to_cve": "27",
    "vulnerable_lines": [
        "// Line_Reference 8: from mealie.pkgs import img",
        "// Line_Reference 35:     async with AsyncClient() as client:",
        "// Line_Reference 37:         responses: list[Response] = await gather_with_concurrency(10, *tasks, ignore_exceptions=True)",
        "// Line_Reference 104:     @staticmethod",
        "// Line_Reference 105:     def _validate_image_url(url: str) -> bool:",
        "// Line_Reference 106:         # sourcery skip: invert-any-all, use-any",
        "// Line_Reference 107:         \"\"\"",
        "// Line_Reference 108:         Validates that the URL is of an allowed source and restricts certain sources to prevent",
        "// Line_Reference 109:         malicious images from being downloaded.",
        "// Line_Reference 110:         \"\"\"",
        "// Line_Reference 111:         invalid_domains = {\"127.0.0.1\", \"localhost\"}",
        "// Line_Reference 112:         for domain in invalid_domains:",
        "// Line_Reference 113:             if domain in url:",
        "// Line_Reference 114:                 return False",
        "// Line_Reference 115: ",
        "// Line_Reference 116:         return True",
        "// Line_Reference 117: ",
        "// Line_Reference 118:     async def scrape_image(self, image_url) -> None:",
        "// Line_Reference 121:         if not self._validate_image_url(image_url):",
        "// Line_Reference 122:             self.logger.error(f\"Invalid image URL: {image_url}\")",
        "// Line_Reference 123:             raise InvalidDomainError(f\"Invalid domain: {image_url}\")",
        "// Line_Reference 126:             pass",
        "// Line_Reference 132:             image_url, _ = await largest_content_len(image_url)",
        "// Line_Reference 137:                     image_url = image_url.get(\"url\")",
        "// Line_Reference 139:         ext = image_url.split(\".\")[-1]",
        "// Line_Reference 147:         async with AsyncClient() as client:",
        "// Line_Reference 149:                 r = await client.get(image_url, headers={\"User-Agent\": _FIREFOX_UA})"
    ]
}
